---
title: "Streaming danych w Node.js"
pubDate: 2026-01-15
description: "Przetwarzanie duÅ¼ych plikÃ³w w Node.js przy uÅ¼yciu streamÃ³w. Jak uniknÄ…Ä‡ przepeÅ‚nienia pamiÄ™ci, poprawiÄ‡ wydajnoÅ›Ä‡ i skalowalnoÅ›Ä‡ backendu."
heroImage: "@assets/blog/streaming-danych-nodejs/hero.png"
keywords:
  [
    "Node.js streams",
    "streaming danych",
    "przetwarzanie duÅ¼ych plikÃ³w",
    "pipeline Node.js",
    "wydajnoÅ›Ä‡ backend",
    "JavaScript backend"
]
tags: [
    "Node.js",
    "Backend",
    "WydajnoÅ›Ä‡",
    "Architektura"
]
---

import bufferVsStream from "@/assets/blog/streaming-danych-nodejs/buffer-vs-stream.png";
import { Picture } from "astro:assets";

## Problem do rozwiÄ…zania

W ekosystemie Node.js, ktÃ³rego architektura opiera siÄ™ na jednowÄ…tkowej pÄ™tli zdarzeÅ„ (Event Loop), efektywne zarzÄ…dzanie I/O jest krytyczne dla stabilnoÅ›ci aplikacji. CzÄ™stym bÅ‚Ä™dem, spotykanym nawet w systemach produkcyjnych, jest traktowanie Node.js jak serwera wielowÄ…tkowego z nieograniczonym stosem pamiÄ™ci.

Objawia siÄ™ to zazwyczaj przy prÃ³bie przetworzenia wiÄ™kszych zestawÃ³w danych (CSV, logi, eksporty JSON), koÅ„czÄ…c siÄ™ bÅ‚Ä™dem `FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed`.


## Koncepcja: Buffer vs Stream

Zanim przejdziemy do kodu, warto zdefiniowaÄ‡ rÃ³Å¼nicÄ™ w utylizacji zasobÃ³w.

* **PodejÅ›cie buforowe**: Wczytuje caÅ‚y zasÃ³b do pamiÄ™ci RAM (Heap) przed rozpoczÄ™ciem przetwarzania. ZÅ‚oÅ¼onoÅ›Ä‡ pamiÄ™ciowa jest liniowa O(n) wzglÄ™dem wielkoÅ›ci pliku.
* **PodejÅ›cie strumieniowe**: Przetwarza dane w maÅ‚ych, zarzÄ…dzalnych porcjach (chunks), zazwyczaj o wielkoÅ›ci 64KB (highWaterMark). UÅ¼ycie pamiÄ™ci jest staÅ‚e, niezaleÅ¼nie od wielkoÅ›ci wejÅ›cia.

> ğŸ’¡ **Uwaga praktyczna:** Parametr `highWaterMark` moÅ¼na dostosowaÄ‡ w zaleÅ¼noÅ›ci od charakteru danych. WiÄ™ksze chunki sprawdzajÄ… siÄ™ przy prostym transferze plikÃ³w, natomiast mniejsze sÄ… korzystne przy transformacjach tekstowych lub przetwarzaniu wymagajÄ…cym niskiego opÃ³Åºnienia.

<figure>
  <Picture data-blog-img src={bufferVsStream} alt="Buffer vs Stream" />
  <figcaption data-figcaption>
    Infografika ilustrujÄ…ca rÃ³Å¼nicÄ™ miÄ™dzy podejÅ›ciem buforowym a strumieniowym w kontekÅ›cie zuÅ¼ycia pamiÄ™ci.
  </figcaption>
</figure>

## Case Study: Endpoint eksportujÄ…cy dane

RozwaÅ¼my scenariusz serwowania duÅ¼ego pliku raportu (np. 1 GB) przez API REST.

PoniÅ¼szy kod jest poprawny syntaktycznie, ale nieakceptowalny architektonicznie dla duÅ¼ych wolumenÃ³w danych.

```ts showLineNumbers title="âŒ ZÅ‚y kod: Buforowanie - alokacja caÅ‚ego pliku w pamiÄ™ci"
import express, { Request, Response } from 'express';
import { readFile } from 'node:fs/promises';

const app = express();

app.get('/download-report', async (req: Request, res: Response) => {
  try {
    // â›” BÅÄ„D: Wysoka presja na pamiÄ™Ä‡ i GC oraz opÃ³Åºnienia w obsÅ‚udze Å¼Ä…daÅ„ przy duÅ¼ym pliku
    // i konsumpcja RAM proporcjonalna do wielkoÅ›ci pliku.
    const data = await readFile('./reports/annual-2026.pdf');
    // WyobraÅºmy sobie, Å¼e plik ma 1 GB - tyle pamiÄ™ci zostanie zablokowane.
    // A co jeÅ›li jednoczeÅ›nie bÄ™dzie 1000 zapytaÅ„? To 1 TB RAM potrzebne do obsÅ‚ugi!

    res.setHeader('Content-Type', 'application/pdf');
    res.send(data);
  } catch (err) {
    res.status(500).send('BÅ‚Ä…d serwera');
  }
});

app.listen(3000, () => console.log("Server running on http://localhost:3000"));
```


Produkcja klasy enterprise wykorzystuje moduÅ‚ node:stream/promises oraz funkcjÄ™ pipeline, ktÃ³ra w TypeScript zapewnia poprawne zarzÄ…dzanie typami oraz automatyczne czyszczenie zasobÃ³w.

```ts showLineNumbers title="âœ… Dobry kod: Strumieniowe przetwarzanie danych"
import express, { Request, Response, NextFunction } from "express";
import { createReadStream } from "node:fs";
import { pipeline } from "node:stream/promises";
import { stat } from "node:fs/promises";
import { join } from "node:path";

const app = express();

app.get(
  "/stream-report",
  async (req: Request, res: Response, next: NextFunction): Promise<void> => {
    const filePath = join(process.cwd(), "reports", "annual-2026.pdf");

    try {
      const fileStats = await stat(filePath);

      res.writeHead(200, {
        "Content-Type": "application/pdf",
        "Content-Length": fileStats.size,
        "Content-Disposition": 'attachment; filename="annual-2026.pdf"',
      });

      const fileStream = createReadStream(filePath);

      // Pipeline automatycznie zarzÄ…dza bÅ‚Ä™dami i zamyka fileStream,
      // jeÅ›li klient rozÅ‚Ä…czy siÄ™ w trakcie pobierania (np. zamknie kartÄ™).
      await pipeline(fileStream, res);
    } catch (err) {
      // JeÅ›li pipeline rzuci bÅ‚Ä…d, musimy sprawdziÄ‡ czy dane juÅ¼ zaczÄ™Å‚y pÅ‚ynÄ…Ä‡
      if (res.headersSent) {
        // JeÅ›li nagÅ‚Ã³wki poszÅ‚y, moÅ¼emy jedynie przerwaÄ‡ poÅ‚Ä…czenie
        res.destroy();
      } else {
        next(err);
      }
    }
  }
);

// Globalny Error Handler
app.use((err: Error, req: Request, res: Response, next: NextFunction) => {
  console.error("Unhandled Stream Error:", err.stack);
  res.status(500).json({ error: "Internal Server Error during streaming" });
});

app.listen(3000, () => console.log("Server running on http://localhost:3000"));
```


## Dlaczego praktycznie zawsze warto uÅ¼ywaÄ‡ streamÃ³w?

1. **ZarzÄ…dzanie zasobami (Resource Cleanup)**: Wykorzystanie pipeline gwarantuje, Å¼e ReadStream zostanie poprawnie zamkniÄ™ty (zniszczony), nawet jeÅ›li proces zapisu do klienta zostanie gwaÅ‚townie przerwany. Zapobiega to wyciekom deskryptorÃ³w plikÃ³w.
2. **StaÅ‚e zuÅ¼ycie pamiÄ™ci**: Strumieniowe przetwarzanie danych pozwala na obsÅ‚ugÄ™ bardzo duÅ¼ych plikÃ³w bez ryzyka wyczerpania pamiÄ™ci RAM serwera.
3. **Lepsza skalowalnoÅ›Ä‡**: Aplikacje oparte na streamach mogÄ… obsÅ‚ugiwaÄ‡ wiÄ™kszÄ… liczbÄ™ jednoczesnych poÅ‚Ä…czeÅ„, poniewaÅ¼ nie blokujÄ… event loop duÅ¼ymi operacjami I/O.
4. **ElastycznoÅ›Ä‡ przetwarzania danych**: Streamy moÅ¼na Å‚atwo Å‚Ä…czyÄ‡ z innymi strumieniami (np. kompresja, szyfrowanie) bez koniecznoÅ›ci buforowania caÅ‚ych plikÃ³w w pamiÄ™ci.
5. **Backpressure (kontrola przepÅ‚ywu danych)**: Streamy w Node.js automatycznie dostosowujÄ… tempo odczytu do moÅ¼liwoÅ›ci odbiorcy. Gdy klient lub kolejny etap pipeline'u nie nadÄ…Å¼a z przetwarzaniem, strumieÅ„ ÅºrÃ³dÅ‚owy zwalnia, zapobiegajÄ…c przepeÅ‚nieniu buforÃ³w i skokom zuÅ¼ycia pamiÄ™ci.

## PrzykÅ‚ad transformacji strumieniowej - kompresja gzip

```ts showLineNumbers
import { createReadStream, createWriteStream } from 'node:fs';
import { createGzip, Gzip } from 'node:zlib';
import { pipeline } from 'node:stream/promises';

async function compressFile(input: string, output: string): Promise<void> {
  const source = createReadStream(input);
  const destination = createWriteStream(output);
  const gzip: Gzip = createGzip();

  // Dane pÅ‚ynÄ… przez transformator gzip, zuÅ¼ywajÄ…c minimalnÄ… iloÅ›Ä‡ pamiÄ™ci
  await pipeline(source, gzip, destination);
}

compressFile('large-log.txt', 'large-log.txt.gz')
  .then(() => console.log('Kompresja zakoÅ„czona'))
  .catch((err) => console.error('BÅ‚Ä…d podczas kompresji:', err));
```

## Podsumowanie

Wykorzystanie strumieni w Node.js to kluczowy wzorzec projektowy dla aplikacji obsÅ‚ugujÄ…cych duÅ¼e iloÅ›ci danych. DziÄ™ki strumieniowemu przetwarzaniu moÅ¼emy efektywnie zarzÄ…dzaÄ‡ zasobami, unikaÄ‡ problemÃ³w z pamiÄ™ciÄ… i budowaÄ‡ skalowalne systemy backendowe. Warto inwestowaÄ‡ czas w naukÄ™ i implementacjÄ™ tego podejÅ›cia, aby zapewniÄ‡ stabilnoÅ›Ä‡ i wydajnoÅ›Ä‡ naszych aplikacji.

